# EV Charging Stations LLM Fine-tuning Pipeline

An end-to-end pipeline for fine-tuning language models on electric vehicle charging station data, with automated data collection, processing, training, evaluation, and deployment.

## ğŸš€ Features

- **Automated Data Collection**: Web scraping and PDF extraction for EV charging data
- **Intelligent Processing**: Cleaning, deduplication, and quality filtering
- **QA Generation**: Automated question-answer pair creation using LLM APIs
- **Memory-Efficient Training**: QLoRA fine-tuning for 7B parameter models
- **Comprehensive Evaluation**: Domain-specific benchmarks and metrics
- **Production Deployment**: FastAPI server with authentication and monitoring
- **MLOps Integration**: CI/CD, orchestration, and monitoring

## ğŸ“‹ Requirements

- Python 3.8+
- CUDA-compatible GPU (for training)
- OpenAI API key (for QA generation)
- HuggingFace access token (for model downloads)

## ğŸ› ï¸ Installation

1. **Clone the repository**:
```bash
git clone <your-repo-url>
cd ml_pipeline
```

2. **Install dependencies**:
```bash
pip install -r requirements.txt
```

3. **Set up environment variables**:
```bash
cp .env.example .env
# Edit .env with your API keys
```

4. **Install Playwright browsers**:
```bash
playwright install
```

## ğŸš€ Quick Start

### 1. Configuration
Edit `config/config.yaml` to customize:
- Domain-specific URLs
- Model parameters
- Training settings
- Evaluation metrics

### 2. Run the Complete Pipeline
```bash
python main.py
```

This will execute:
- Data collection from EV charging websites
- Data processing and cleaning
- QA pair generation
- Model fine-tuning with QLoRA
- Evaluation and benchmarking
- Model registration for deployment

### 3. Deploy the Model
```bash
# Start the inference server
uvicorn src.deployment.inference_server:app --host 0.0.0.0 --port 8000

# Test the API
curl -X POST "http://localhost:8000/infer" \
  -H "api-key: test-key" \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "What is Level 2 charging?",
    "adapter_version": "v1.0",
    "base_model": "meta-llama/Llama-2-7b-hf"
  }'
```

## ğŸ“ Project Structure

```
ml_pipeline/
â”œâ”€â”€ config/                 # Configuration files
â”‚   â”œâ”€â”€ settings.py        # Pydantic settings with env support
â”‚   â””â”€â”€ config.yaml        # Domain-specific configuration
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data_collection/   # Web scraping, PDF extraction
â”‚   â”œâ”€â”€ data_processing/   # Cleaning, filtering, storage
â”‚   â”œâ”€â”€ training/          # Model loading, LoRA, training loop
â”‚   â”œâ”€â”€ evaluation/        # Benchmarks, metrics, comparison
â”‚   â”œâ”€â”€ deployment/        # FastAPI server, model registry
â”‚   â””â”€â”€ orchestration/     # Workflow automation
â”œâ”€â”€ tests/                 # Unit and integration tests
â”œâ”€â”€ docker/               # Containerization files
â”œâ”€â”€ main.py               # End-to-end pipeline
â””â”€â”€ requirements.txt      # Dependencies
```

## ğŸ”§ Configuration

### Environment Variables
```bash
OPENAI_API_KEY=sk-...          # For QA generation
WANDB_API_KEY=...              # For experiment tracking
SLACK_API_TOKEN=...            # For failure alerts
SLACK_CHANNEL=#general         # Slack channel for alerts
```

### Model Configuration
- **Base Model**: Llama-2-7B (configurable)
- **Fine-tuning**: QLoRA with rank 16, alpha 32
- **Quantization**: 4-bit for memory efficiency
- **Training**: Gradient accumulation, mixed precision

## ğŸ“Š Evaluation

The pipeline includes comprehensive evaluation:

- **Domain-specific benchmarks**: Auto-generated EV charging questions
- **Standard metrics**: ROUGE, BLEU, F1, Exact Match
- **Performance metrics**: Latency, throughput
- **Baseline comparison**: Fine-tuned vs base model

## ğŸš€ Deployment

### Docker Deployment
```bash
# Build and run with Docker
docker-compose -f docker/docker-compose.yml up --build
```

### Production Considerations
- Set up proper API keys and authentication
- Configure monitoring and alerting
- Use GPU-enabled containers for inference
- Implement rate limiting and security

## ğŸ§ª Testing

Run the test suite:
```bash
# Unit tests
pytest tests/

# Integration tests
pytest tests/test_integration.py

# Full pipeline test (requires API keys)
python main.py
```

## ğŸ“ˆ Monitoring

- **Experiment Tracking**: Weights & Biases integration
- **API Monitoring**: Prometheus metrics
- **Pipeline Monitoring**: Slack alerts for failures
- **Performance Monitoring**: Latency and throughput tracking

## ğŸ”„ CI/CD

The pipeline includes GitHub Actions for:
- Automated testing
- Code quality checks
- Docker image building
- Deployment automation

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Add tests for new functionality
4. Submit a pull request

## ğŸ“„ License

MIT License - see LICENSE file for details

## ğŸ†˜ Support

For issues and questions:
- Check the documentation
- Review the test suite
- Open an issue on GitHub

## ğŸ¯ Domain Customization

To adapt this pipeline for other domains:

1. Update `config/config.yaml` with domain-specific URLs and prompts
2. Modify the QA generation prompts in `src/training/dataset_preparation.py`
3. Update benchmark questions in `src/evaluation/benchmark_creator.py`
4. Adjust data collection URLs in `main.py`

## ğŸ“Š Performance

Typical performance metrics:
- **Training Time**: 2-4 hours on RTX 4090
- **Inference Latency**: <500ms per request
- **Memory Usage**: ~8GB GPU memory during training
- **Model Size**: ~4GB (4-bit quantized)

## ğŸ”® Future Enhancements

- Multi-modal support (images, diagrams)
- Real-time data collection
- Advanced retrieval-augmented generation
- A/B testing framework
- Auto-scaling deployment 